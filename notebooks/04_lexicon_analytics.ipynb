{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04_lexicon_analytics.ipynb\n",
        "\n",
        "Explore dish descriptions to surface information for building a lexicon (taste, aroma, texture)\n",
        "without committing to rules yet. This notebook produces frequency tables, collocations,\n",
        "co-occurrence matrices, TF-IDF terms, and KWIC samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import islice\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "if ROOT.name == 'notebooks' and (ROOT.parent / 'data_cleaned').exists():\n",
        "    ROOT = ROOT.parent\n",
        "DATA_CLEANED = ROOT / 'data_cleaned'\n",
        "\n",
        "PREFS = [\n",
        "    DATA_CLEANED / 'user_orders_desc_clean.csv',\n",
        "    DATA_CLEANED / 'user_orders_clean_with_description.csv',\n",
        "    Path('/mnt/data/final.csv'),\n",
        "]\n",
        "INPUT = None\n",
        "for p in PREFS:\n",
        "    if p.exists():\n",
        "        INPUT = p\n",
        "        break\n",
        "assert INPUT is not None, 'Could not find an input CSV.'\n",
        "print('Using input:', INPUT)\n",
        "\n",
        "df = pd.read_csv(INPUT)\n",
        "col_candidates = [c for c in df.columns if c.lower().strip() == 'dish_description']\n",
        "assert col_candidates, 'Input must contain a dish_description column.'\n",
        "DESC_COL = col_candidates[0]\n",
        "print('Description column =', DESC_COL, '| Rows =', len(df))\n",
        "\n",
        "OUT_DIR = DATA_CLEANED\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "STOPWORDS = set('''a an and the of to in on with for by at from as is are be or that this those these it its into over under out up down if then than while when where which who whom whose also often usually typically commonly likely may might can could should would such just very more less most least light mild heavy extra hint touch rich loaded fresh classic style served'''.split())\n",
        "\n",
        "def normalize(s: str) -> str:\n",
        "    s = str(s).lower()\n",
        "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def tokenize(s: str):\n",
        "    t = normalize(s)\n",
        "    toks = [w for w in t.split() if len(w) > 1 and w not in STOPWORDS]\n",
        "    return toks\n",
        "\n",
        "def ngrams(words, n):\n",
        "    return [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "docs = df[DESC_COL].fillna('').astype(str).tolist()\n",
        "doc_tokens = [tokenize(s) for s in docs]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Global frequencies\n",
        "tok_freq = Counter()\n",
        "bigram_freq = Counter()\n",
        "trigram_freq = Counter()\n",
        "\n",
        "for toks in doc_tokens:\n",
        "    tok_freq.update(toks)\n",
        "    bigram_freq.update(ngrams(toks, 2))\n",
        "    trigram_freq.update(ngrams(toks, 3))\n",
        "\n",
        "tokens_top = pd.DataFrame(tok_freq.most_common(500), columns=['token','count'])\n",
        "bigrams_top = pd.DataFrame([(\" \".join(k), v) for k, v in bigram_freq.most_common(300)], columns=['bigram','count'])\n",
        "trigrams_top = pd.DataFrame([(\" \".join(k), v) for k, v in trigram_freq.most_common(200)], columns=['trigram','count'])\n",
        "\n",
        "tokens_top.to_csv(OUT_DIR / 'tokens_top.csv', index=False)\n",
        "bigrams_top.to_csv(OUT_DIR / 'bigrams_top.csv', index=False)\n",
        "trigrams_top.to_csv(OUT_DIR / 'trigrams_top.csv', index=False)\n",
        "tokens_top.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Collocations from patterns observed in your data\n",
        "patterns = {\n",
        "    'with_X': re.compile(r\"\\bwith\\s+([a-z]+)\\b\"),\n",
        "    'in_X_sauce': re.compile(r\"\\bin\\s+([a-z]+)\\s+sauce\\b\"),\n",
        "    'fried_X': re.compile(r\"\\bfried\\s+([a-z]+)\\b\"),\n",
        "    'grilled_X': re.compile(r\"\\bgrilled\\s+([a-z]+)\\b\"),\n",
        "    'garlic_X': re.compile(r\"\\bgarlic\\s+([a-z]+)\\b\"),\n",
        "}\n",
        "colloc = {k: Counter() for k in patterns}\n",
        "for s in docs:\n",
        "    s = s.lower()\n",
        "    for name, pat in patterns.items():\n",
        "        for m in pat.finditer(s):\n",
        "            colloc[name][m.group(1)] += 1\n",
        "\n",
        "for name, cnt in colloc.items():\n",
        "    out = pd.DataFrame(cnt.most_common(200), columns=['x','count'])\n",
        "    out.to_csv(OUT_DIR / f'collocations_{name}.csv', index=False)\n",
        "\n",
        "pd.DataFrame(colloc['with_X'].most_common(20), columns=['x','count']).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cue Ã— Ingredient co-occurrence matrix (dataset-limited)\n",
        "cue_words = {\n",
        "    'taste': ['sweet','spicy','sour','salty','umami','bitter','tangy'],\n",
        "    'aroma': ['garlic','garlicky','buttery','smoky','grilled','smoked','bbq','aioli'],\n",
        "    'texture': ['crispy','crunchy','creamy','juicy','tender','soft','fried']\n",
        "}\n",
        "ingredients = ['cheese','cheddar','bacon','beef','chicken','mushroom','tomato','lettuce','pickles','onion','ketchup','mustard','ranch','gravy','mayo','aioli','bbq','bun','fries']\n",
        "\n",
        "cue_list = sorted({w for cat in cue_words.values() for w in cat})\n",
        "ing_list = sorted(set(ingredients))\n",
        "\n",
        "cooc = pd.DataFrame(0, index=cue_list, columns=ing_list, dtype=int)\n",
        "\n",
        "for toks in doc_tokens:\n",
        "    ts = set(toks)\n",
        "    cues_present = ts & set(cue_list)\n",
        "    ings_present = ts & set(ing_list)\n",
        "    for c in cues_present:\n",
        "        for ig in ings_present:\n",
        "            cooc.loc[c, ig] += 1\n",
        "\n",
        "cooc.to_csv(OUT_DIR / 'cooccurrence_matrix.csv')\n",
        "cooc.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TF-IDF key terms per document (if scikit-learn available), exported for auditing\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    vec = TfidfVectorizer(\n",
        "        tokenizer=lambda s: tokenize(s),\n",
        "        preprocessor=lambda s: s,\n",
        "        lowercase=False,\n",
        "        min_df=2,\n",
        "        max_df=0.9,\n",
        "        ngram_range=(1,2)\n",
        "    )\n",
        "    X = vec.fit_transform(df[DESC_COL].fillna('').astype(str).tolist())\n",
        "    terms = np.array(vec.get_feature_names_out())\n",
        "    # For each doc: take top 8 terms\n",
        "    tops = []\n",
        "    for i in range(X.shape[0]):\n",
        "        row = X.getrow(i)\n",
        "        if row.nnz == 0:\n",
        "            tops.append(\"\")\n",
        "            continue\n",
        "        idx = row.indices\n",
        "        vals = row.data\n",
        "        order = np.argsort(-vals)[:8]\n",
        "        tops.append(\", \".join(terms[idx[order]]))\n",
        "    tfidf_df = df.copy()\n",
        "    tfidf_df['tfidf_top_terms'] = tops\n",
        "    tfidf_df[['tfidf_top_terms']].to_csv(OUT_DIR / 'tfidf_terms_by_doc.csv', index=False)\n",
        "    print('TF-IDF terms written.')\n",
        "except Exception as e:\n",
        "    print('Skipped TF-IDF (scikit-learn not available or failed):', e)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# KWIC (Key Word In Context) sampling utility\n",
        "def kwic(samples, keyword, window=40, max_hits=30):\n",
        "    out = []\n",
        "    pat = re.compile(r\"(.{0,%d}\\b%s\\b.{0,%d})\" % (window, re.escape(keyword), window), re.I)\n",
        "    for s in samples:\n",
        "        for m in pat.finditer(s):\n",
        "            out.append(m.group(1).replace('\\n',' '))\n",
        "            if len(out) >= max_hits:\n",
        "                return out\n",
        "    return out\n",
        "\n",
        "# Generate a small KWIC file for common cues\n",
        "cues_for_kwic = ['spicy','crispy','creamy','garlic','bbq','ranch','ketchup','mustard','gravy','aioli']\n",
        "lines = []\n",
        "for k in cues_for_kwic:\n",
        "    hits = kwic(df[DESC_COL].fillna('').astype(str).tolist(), k, window=50, max_hits=20)\n",
        "    lines.append(f'===== {k} =====')\n",
        "    lines.extend(hits if hits else ['(no matches)'])\n",
        "    lines.append('')\n",
        "(OUT_DIR / 'kwic_samples.txt').write_text('\\n'.join(lines))\n",
        "print('KWIC written:', OUT_DIR / 'kwic_samples.txt')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}