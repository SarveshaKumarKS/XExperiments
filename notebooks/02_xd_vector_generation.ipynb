{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_xd_vector_generation.ipynb\n",
    "\n",
    "Build a dataset-grounded **lexicon** (Taste/Aroma/Texture) from\n",
    "`dish_name_to_be_processed` and generate the **XD flavor vectors**.\n",
    "\n",
    "Inputs: `project_x/data_cleaned/user_orders_clean.csv`\n",
    "Outputs: `lexicon.yaml`, `xd_vectors_32bit.csv`, `xd_report.txt` in `data_cleaned/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01municodedata\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01myaml\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, sys, unicodedata, yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == 'notebooks' and (cwd.parent / 'data_cleaned').exists():\n",
    "    ROOT = cwd.parent\n",
    "else:\n",
    "    search = cwd\n",
    "    ROOT = None\n",
    "    for _ in range(5):\n",
    "        if (search / 'data_cleaned').exists():\n",
    "            ROOT = search\n",
    "            break\n",
    "        search = search.parent\n",
    "    if ROOT is None:\n",
    "        ROOT = cwd\n",
    "\n",
    "DATA_CLEANED = ROOT / 'data_cleaned'\n",
    "INPUT_CSV = DATA_CLEANED / 'user_orders_clean.csv'\n",
    "LEXICON_YAML = DATA_CLEANED / 'lexicon.yaml'\n",
    "XD_CSV = DATA_CLEANED / 'xd_vectors_32bit.csv'\n",
    "REPORT = DATA_CLEANED / 'xd_report.txt'\n",
    "\n",
    "print('ROOT:', ROOT)\n",
    "print('INPUT_CSV:', INPUT_CSV)\n",
    "assert INPUT_CSV.exists(), f\"Missing {INPUT_CSV}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_CSV)\n",
    "assert 'dish_name_to_be_processed' in df.columns, \"Expected 'dish_name_to_be_processed' column.\"\n",
    "print('Rows:', len(df))\n",
    "df[['dish_name_to_be_processed']].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s: str) -> str:\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n",
    "\n",
    "def tokenize(text: str):\n",
    "    t = strip_accents(str(text).lower())\n",
    "    t = re.sub(r\"[^a-z\\s]\", \" \", t)\n",
    "    return [w for w in t.split() if w]\n",
    "\n",
    "all_tokens, rows_tokens = [], []\n",
    "for s in df['dish_name_to_be_processed'].fillna(''):\n",
    "    toks = tokenize(s)\n",
    "    rows_tokens.append(toks)\n",
    "    all_tokens.extend(toks)\n",
    "token_freq = Counter(all_tokens)\n",
    "print('Unique tokens:', len(token_freq))\n",
    "pd.DataFrame(token_freq.most_common(30), columns=['token','count']).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed â†’ restrict to dataset tokens only \n",
    "seed_taste = {\n",
    "    'sweet': {'sweet','sugar','chocolate','caramel','honey','cake','milkshake','lemonade','cookie','brownie','icing','frosting'},\n",
    "    'spicy': {'spicy','hot','chili','jalapeno','pepper','hotshot'},\n",
    "    'sour':  {'tamarind','lemon','lime','sour','tangy','vinegar'},\n",
    "    'salty': {'salt','salty','fries','chips'},\n",
    "    'umami': {'umami','cheese','cheesy','cheesesteak','burger','beef','chicken','gravy','mushroom','soy','fish','nuggets','biryani'},\n",
    "    'bitter':{'bitter','coffee','dark'},\n",
    "}\n",
    "seed_aroma = {\n",
    "    'garlic':  {'garlic','onion'},\n",
    "    'buttery': {'butter','buttery','creamy','milk'},\n",
    "    'smoky':   {'smoky','smoked','bbq','grilled'},\n",
    "    'spiced':  {'pepper','curry','masala','manchurian'},\n",
    "    'sweet_aroma': {'vanilla','chocolate'},\n",
    "    'citrus':  {'lemon','lime','orange','peach','wildberry'},\n",
    "}\n",
    "seed_texture = {\n",
    "    'crispy': {'crispy','crunchy','fried','fries','chips','nuggets','rings'},\n",
    "    'creamy': {'creamy','sauce','gravy','milk','cheese'},\n",
    "    'soft':   {'soft','mashed','bun','bread'},\n",
    "    'chewy':  {'chewy'},\n",
    "}\n",
    "\n",
    "token_set = set(token_freq)\n",
    "def restrict(seed):\n",
    "    out = {}\n",
    "    for k,v in seed.items():\n",
    "        keep = sorted(set(v) & token_set)\n",
    "        if keep:\n",
    "            out[k] = keep\n",
    "    return out\n",
    "\n",
    "lexicon = {'taste': restrict(seed_taste), 'aroma': restrict(seed_aroma), 'texture': restrict(seed_texture)}\n",
    "with open(ROOT / 'data_cleaned' / 'lexicon.yaml', 'w') as f:\n",
    "    yaml.safe_dump(lexicon, f, sort_keys=False)\n",
    "lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build binary XD vector\n",
    "xd_dims = []\n",
    "for cat, subs in lexicon.items():\n",
    "    for sub in subs:\n",
    "        xd_dims.append(f\"{cat}_{sub}\")\n",
    "\n",
    "def score(tokens):\n",
    "    s = {d:0 for d in xd_dims}\n",
    "    ts = set(tokens)\n",
    "    for cat, subs in lexicon.items():\n",
    "        for sub, words in subs.items():\n",
    "            if ts & set(words):\n",
    "                s[f\"{cat}_{sub}\"] = 1\n",
    "    return s\n",
    "\n",
    "rows = [score(t) for t in rows_tokens]\n",
    "xd_df = pd.DataFrame(rows, dtype=np.float32)\n",
    "xd_df.index = df.index\n",
    "out = pd.concat([df[['dish_name_to_be_processed']], xd_df], axis=1)\n",
    "out.to_csv(XD_CSV, index=False)\n",
    "out.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report\n",
    "lines = []\n",
    "lines.append(f\"Rows: {len(df)}\")\n",
    "lines.append(f\"XD dims: {len(xd_dims)}\")\n",
    "lines.append(\"\\nFirst 10 rows with positive dims:\")\n",
    "for i, r in out.head(10).iterrows():\n",
    "    pos = [d for d in xd_dims if int(r.get(d,0))==1]\n",
    "    lines.append(f\" - {r['dish_name_to_be_processed']} => {pos}\")\n",
    "\n",
    "Path(REPORT).write_text(\"\\n\".join(lines))\n",
    "print('Wrote', REPORT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
