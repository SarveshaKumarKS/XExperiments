{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02b_llm_descriptions_only_v2.ipynb\n",
        "\n",
        "Generate **dish_description** (no scoring) for each row using OpenAI, based on\n",
        "`dish_name`, `dish_name_to_be_processed`, `restaurant_name`, and `city_name`.\n",
        "\n",
        "### Inputs\n",
        "- `project_x/data_cleaned/user_orders_clean.csv`\n",
        "\n",
        "### Outputs\n",
        "- `project_x/data_cleaned/user_orders_clean_with_description.csv`\n",
        "- Cache (versioned): `project_x/data_cleaned/llm_run_cache__{PROMPT_VERSION}.jsonl`\n",
        "- Coverage summary printed at the end\n",
        "\n",
        "**Note:** Set the `OPENAI_API_KEY` in your environment (or use a `.env`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROOT: /Users/sarveshaks/Documents/Documents - Sarvesha’s MacBook Pro (2)/Project Stea/project_x\n",
            "INPUT_CSV: /Users/sarveshaks/Documents/Documents - Sarvesha’s MacBook Pro (2)/Project Stea/project_x/data_cleaned/user_orders_clean.csv\n",
            "OUTPUT_CSV: /Users/sarveshaks/Documents/Documents - Sarvesha’s MacBook Pro (2)/Project Stea/project_x/data_cleaned/user_orders_clean_with_description.csv\n"
          ]
        }
      ],
      "source": [
        "# Optional: one-time installs (uncomment if needed)\n",
        "# %pip install openai==1.51.0 pandas numpy tqdm tenacity python-dotenv\n",
        "from pathlib import Path\n",
        "import os, json, time, hashlib\n",
        "from typing import Dict, Any\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load .env if present\n",
        "load_dotenv()\n",
        "\n",
        "# Project root autodetect\n",
        "cwd = Path.cwd()\n",
        "if cwd.name == 'notebooks' and (cwd.parent / 'data_cleaned').exists():\n",
        "    ROOT = cwd.parent\n",
        "else:\n",
        "    search = cwd\n",
        "    ROOT = None\n",
        "    for _ in range(5):\n",
        "        if (search / 'data_cleaned').exists():\n",
        "            ROOT = search\n",
        "            break\n",
        "        search = search.parent\n",
        "    if ROOT is None:\n",
        "        ROOT = cwd\n",
        "\n",
        "DATA_CLEANED = ROOT / 'data_cleaned'\n",
        "INPUT_CSV = DATA_CLEANED / 'user_orders_clean.csv'\n",
        "OUTPUT_CSV = DATA_CLEANED / 'user_orders_clean_with_description.csv'\n",
        "\n",
        "print('ROOT:', ROOT)\n",
        "print('INPUT_CSV:', INPUT_CSV)\n",
        "print('OUTPUT_CSV:', OUTPUT_CSV)\n",
        "assert INPUT_CSV.exists(), f\"Missing input CSV at {INPUT_CSV}\"\n",
        "\n",
        "# OpenAI client\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "assert OPENAI_API_KEY, 'OPENAI_API_KEY not set. Use a .env or export the variable.'\n",
        "MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\n",
        "TEMPERATURE = float(os.getenv('OPENAI_TEMPERATURE', '0.2'))\n",
        "BATCH_SIZE = 10\n",
        "ROW_LIMIT = int(os.getenv('ROW_LIMIT', '0'))  # 0 means no explicit limit\n",
        "CHECKPOINT_INTERVAL = 50\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CACHE_PATH: /Users/sarveshaks/Documents/Documents - Sarvesha’s MacBook Pro (2)/Project Stea/project_x/data_cleaned/llm_run_cache__flavor-v2.3.jsonl\n"
          ]
        }
      ],
      "source": [
        "# === Prompt versioning & helpers ===\n",
        "PROMPT_VERSION = 'flavor-v2.3'  # bump this when you change rules\n",
        "\n",
        "def sha256(text: str) -> str:\n",
        "    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
        "\n",
        "# Versioned cache file per prompt version\n",
        "CACHE_PATH = DATA_CLEANED / f\"llm_run_cache__{PROMPT_VERSION}.jsonl\"\n",
        "print('CACHE_PATH:', CACHE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SYSTEM_PROMPT hash: 6c3a13ed591a\n"
          ]
        }
      ],
      "source": [
        "# === System prompt (v2) with salt/version header ===\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a culinary analyst. Given basic fields about a menu item, produce a single JSON object with EXACTLY one key: 'description'. \"\n",
        "    \"The 'description' must be 40–65 words and ALWAYS include all three sensory dimensions — taste, aroma, and texture. \"\n",
        "    \"Use clear, natural language to create one compact sentence. \\n\\n\"\n",
        "    \"• Taste cues: sweet, salty, sour, bitter, umami/savory, spicy/heat, peppery, tangy, bittersweet, smoky-sweet, honeyed, caramelized. \\n\"\n",
        "    \"• Aroma cues: go beyond 'garlicky/buttery/smoky/citrusy'; also use oniony, nutty, toasty, roasted, charred, woody, earthy, floral, herby/herbal \"\n",
        "    \"(basil, oregano, thyme, rosemary, dill, mint), warm-spice (cumin, coriander, fennel, anise, clove, cardamom, cinnamon), sesame, peanutty, coconutty, lemongrass, kaffir-lime, vinegar-sharp. \\n\"\n",
        "    \"• Texture cues: crispy, crunchy, crackly, airy, flaky, tender, succulent/juicy, springy, bouncy, toothsome/al dente, silky/smooth/velvety, thick, coarse, crumbly, sticky, gooey, saucy, glaze-coated. \\n\\n\"\n",
        "    \"Evenness requirement: Each description MUST contain at least one cue from EACH of taste, aroma, and texture. \"\n",
        "    \"Prefer 1–3 cues per dimension, and vary vocabulary across items so the same aroma term isn’t reused excessively. \"\n",
        "    \"If exact properties are uncertain, infer typical ones from cuisine, ingredients, or cooking method using hedges like 'likely', 'typically', or 'commonly'. \\n\\n\"\n",
        "    \"Rules: Use only generally known culinary knowledge; do not invent brand-specific or proprietary details. \"\n",
        "    \"Keep tone neutral, concise, and cuisine-aware. Mention sides or serving style only if space allows after all three sensory dimensions are covered. \\n\\n\"\n",
        "    \"Output rules: Return ONLY valid JSON with a single key 'description' and a single string value. No markdown, no extra keys, no comments. \"\n",
        "    \"The string must be one grammatical sentence that explicitly includes at least one taste cue, one aroma cue, and one texture cue as defined above. \"\n",
        "    \"Avoid overusing 'garlicky', 'buttery', 'smoky', or 'citrusy' unless most natural; use broader, varied vocabulary.\"\n",
        ")\n",
        "\n",
        "# prepend version tag to ensure upstream caches miss\n",
        "SYSTEM_PROMPT = f\"[{PROMPT_VERSION}]\\n\" + SYSTEM_PROMPT.strip()\n",
        "print('SYSTEM_PROMPT hash:', sha256(SYSTEM_PROMPT)[:12])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Validator: every description must have taste + aroma + texture ===\n",
        "TASTE  = {\"sweet\",\"salty\",\"sour\",\"bitter\",\"umami\",\"savory\",\"spicy\",\"heat\",\"peppery\",\"tangy\",\"bittersweet\",\"smoky\",\"smoky-sweet\",\"caramelized\",\"honeyed\"}\n",
        "AROMA  = {\"garlic\",\"garlicky\",\"onion\",\"oniony\",\"butter\",\"buttery\",\"nutty\",\"toasty\",\"roasted\",\"charred\",\"woody\",\"earthy\",\"floral\",\n",
        "          \"herb\",\"herbal\",\"basil\",\"oregano\",\"thyme\",\"rosemary\",\"dill\",\"mint\",\"cumin\",\"coriander\",\"fennel\",\"anise\",\"clove\",\"cardamom\",\n",
        "          \"cinnamon\",\"sesame\",\"peanut\",\"peanutty\",\"coconut\",\"coconutty\",\"lemongrass\",\"kaffir\",\"lime\",\"vinegar\",\"fragrant\",\"aroma\",\"paprika\",\"ginger\",\"turmeric\",\"curry\"}\n",
        "TEXTURE= {\"crispy\",\"crunchy\",\"crackly\",\"airy\",\"flaky\",\"tender\",\"juicy\",\"succulent\",\"springy\",\"bouncy\",\"toothsome\",\"al dente\",\n",
        "          \"silky\",\"smooth\",\"velvety\",\"thick\",\"coarse\",\"crumbly\",\"sticky\",\"gooey\",\"saucy\",\"glaze\",\"glaze-coated\",\"creamy\",\"chewy\"}\n",
        "\n",
        "def _has_any(text: str, vocab: set) -> bool:\n",
        "    t = (text or '').lower()\n",
        "    return any(v in t for v in vocab)\n",
        "\n",
        "def valid_description(desc: str) -> bool:\n",
        "    return _has_any(desc, TASTE) and _has_any(desc, AROMA) and _has_any(desc, TEXTURE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows in CSV: 382\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dish_name</th>\n",
              "      <th>dish_name_to_be_processed</th>\n",
              "      <th>restaurant_name</th>\n",
              "      <th>city_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Chicken shawarma gyro</td>\n",
              "      <td>chicken shawarma spicy sauce</td>\n",
              "      <td>Gyro Kingdom (NE Davis)</td>\n",
              "      <td>Portland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crispy Fries</td>\n",
              "      <td>crispy fries</td>\n",
              "      <td>Gyro Kingdom (NE Davis)</td>\n",
              "      <td>Portland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Chicken Shawarma Plate</td>\n",
              "      <td>chicken shawarma plate</td>\n",
              "      <td>Gyro Kingdom (NE Davis)</td>\n",
              "      <td>Portland</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                dish_name     dish_name_to_be_processed  \\\n",
              "0   Chicken shawarma gyro  chicken shawarma spicy sauce   \n",
              "1            Crispy Fries                  crispy fries   \n",
              "2  Chicken Shawarma Plate        chicken shawarma plate   \n",
              "\n",
              "           restaurant_name city_name  \n",
              "0  Gyro Kingdom (NE Davis)  Portland  \n",
              "1  Gyro Kingdom (NE Davis)  Portland  \n",
              "2  Gyro Kingdom (NE Davis)  Portland  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === Data load ===\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "needed = ['dish_name','dish_name_to_be_processed','restaurant_name','city_name']\n",
        "for c in needed:\n",
        "    assert c in df.columns, f\"Missing column: {c}\"\n",
        "\n",
        "if 'dish_description' not in df.columns:\n",
        "    df['dish_description'] = np.nan\n",
        "\n",
        "print('Rows in CSV:', len(df))\n",
        "display(df[needed].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded cache items: 0\n"
          ]
        }
      ],
      "source": [
        "# === Cache loader (versioned) ===\n",
        "cache = {}\n",
        "if CACHE_PATH.exists():\n",
        "    with open(CACHE_PATH, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "                cache[obj['cache_key']] = obj\n",
        "            except Exception:\n",
        "                pass\n",
        "print('Loaded cache items:', len(cache))\n",
        "\n",
        "def make_cache_key(row: pd.Series, system_prompt: str) -> str:\n",
        "    base = {\n",
        "        'v': PROMPT_VERSION,\n",
        "        'model': MODEL,\n",
        "        'system_hash': sha256(system_prompt),\n",
        "        'dish_name': str(row['dish_name']),\n",
        "        'dish_name_to_be_processed': str(row['dish_name_to_be_processed']),\n",
        "        'restaurant_name': str(row['restaurant_name']),\n",
        "        'city_name': str(row['city_name'])\n",
        "    }\n",
        "    return json.dumps(base, sort_keys=True, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Prompt builder & parsing ===\n",
        "def build_user_prompt(row: pd.Series) -> str:\n",
        "    payload = {\n",
        "        'dish_name': str(row['dish_name']),\n",
        "        'dish_name_to_be_processed': str(row['dish_name_to_be_processed']),\n",
        "        'restaurant_name': str(row['restaurant_name']),\n",
        "        'city_name': str(row['city_name'])\n",
        "    }\n",
        "    return (\n",
        "        \"Use the following fields to infer a realistic culinary description.\\n\" +\n",
        "        json.dumps(payload, ensure_ascii=False)\n",
        "    )\n",
        "\n",
        "def safe_parse_json(text: str) -> Dict[str, Any]:\n",
        "    try:\n",
        "        obj = json.loads(text)\n",
        "        if isinstance(obj, dict) and 'description' in obj and isinstance(obj['description'], str):\n",
        "            return obj\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {'description': ''}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === OpenAI call with retry ===\n",
        "@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=1, max=30), reraise=True)\n",
        "def call_llm(system_prompt: str, user_prompt: str) -> Dict[str, Any]:\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        temperature=TEMPERATURE,\n",
        "        messages=[{\"role\":\"system\",\"content\":system_prompt}, {\"role\":\"user\",\"content\":user_prompt}],\n",
        "    )\n",
        "    text = resp.choices[0].message.content\n",
        "    return safe_parse_json(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Planned rows this run: 382 (of 382)\n"
          ]
        }
      ],
      "source": [
        "# === Worklist: only rows with missing/empty description (or limit via ROW_LIMIT) ===\n",
        "needs = df[(df['dish_description'].isna()) | (df['dish_description'].astype(str).str.len() == 0)].copy()\n",
        "if ROW_LIMIT > 0:\n",
        "    needs = needs.head(ROW_LIMIT)\n",
        "rows = needs.index.tolist()\n",
        "print(f\"Planned rows this run: {len(rows)} (of {len(df)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/39 [00:00<?, ?it/s]/var/folders/gq/_54kskxn69x7bbbx_s277bdw0000gn/T/ipykernel_6543/3853061904.py:41: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The chicken shawarma gyro offers a savory taste with a hint of spice, complemented by the warm, herby aroma of cumin and coriander, while its tender, juicy texture is enveloped in a slightly crispy pita, creating a delightful contrast that enhances each flavorful bite.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  df.at[idx, 'dish_description'] = desc\n",
            " 13%|█▎        | 5/39 [02:00<13:30, 23.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at 50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 10/39 [03:25<08:49, 18.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 15/39 [04:53<07:02, 17.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at 150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████▏    | 20/39 [06:28<05:53, 18.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at 200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 25/39 [08:02<04:20, 18.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 30/39 [09:37<02:46, 18.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at 300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|████████▉ | 35/39 [11:24<01:23, 20.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at 350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [12:40<00:00, 19.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed rows: 382\n",
            "Final CSV written to: /Users/sarveshaks/Documents/Documents - Sarvesha’s MacBook Pro (2)/Project Stea/project_x/data_cleaned/user_orders_clean_with_description.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# === Main loop: cache-aware + validator + salted retries ===\n",
        "processed = 0\n",
        "DATA_CLEANED.mkdir(parents=True, exist_ok=True)\n",
        "with open(CACHE_PATH, 'a', encoding='utf-8') as cache_f:\n",
        "    for i in tqdm(range(0, len(rows), BATCH_SIZE)):\n",
        "        batch_idx = rows[i:i+BATCH_SIZE]\n",
        "        for idx in batch_idx:\n",
        "            row = df.loc[idx]\n",
        "            key = make_cache_key(row, SYSTEM_PROMPT)\n",
        "            result = None\n",
        "\n",
        "            # 1) Try cache first, but re-validate\n",
        "            cached = cache.get(key)\n",
        "            if cached:\n",
        "                cand = (cached.get('result') or {}).get('description', '')\n",
        "                if cand and valid_description(cand):\n",
        "                    result = cached['result']\n",
        "\n",
        "            # 2) If no valid cache, generate with up to 3 salted retries\n",
        "            if result is None:\n",
        "                up = build_user_prompt(row)\n",
        "                result = {'description': ''}\n",
        "                for attempt in range(1, 4):\n",
        "                    try:\n",
        "                        salted = SYSTEM_PROMPT + f\"\\n[retry_hint={attempt}:{int(time.time())%100000}]\"\n",
        "                        r = call_llm(salted, up)\n",
        "                        desc_try = (r.get('description') or '').strip()\n",
        "                        if valid_description(desc_try):\n",
        "                            result = r\n",
        "                            break\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                cache_line = {'cache_key': key, 'result': result}\n",
        "                cache_f.write(json.dumps(cache_line, ensure_ascii=False) + \"\\n\")\n",
        "                cache[key] = cache_line\n",
        "\n",
        "            # 3) Assign if present\n",
        "            desc = (result.get('description') or '').strip()\n",
        "            if desc:\n",
        "                df.at[idx, 'dish_description'] = desc\n",
        "\n",
        "        processed += len(batch_idx)\n",
        "        # periodic checkpoint\n",
        "        if processed % CHECKPOINT_INTERVAL == 0:\n",
        "            df.to_csv(OUTPUT_CSV, index=False)\n",
        "            print('Checkpoint saved at', processed)\n",
        "\n",
        "print('Processed rows:', processed)\n",
        "df.to_csv(OUTPUT_CSV, index=False)\n",
        "print('Final CSV written to:', OUTPUT_CSV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coverage: {'rows': 382, 'taste≥1': 363, 'aroma≥1': 363, 'texture≥1': 363, 'all_three': 363, 'all_three_pct': 95.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rows': 382,\n",
              " 'taste≥1': 363,\n",
              " 'aroma≥1': 363,\n",
              " 'texture≥1': 363,\n",
              " 'all_three': 363,\n",
              " 'all_three_pct': 95.0}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Coverage summary (great for slides) ===\n",
        "def _has_any_series(series: pd.Series, vocab: set) -> int:\n",
        "    return series.fillna('').str.lower().apply(lambda s: any(v in s for v in vocab)).sum()\n",
        "\n",
        "total = len(df)\n",
        "taste_n   = _has_any_series(df['dish_description'], TASTE)\n",
        "aroma_n   = _has_any_series(df['dish_description'], AROMA)\n",
        "texture_n = _has_any_series(df['dish_description'], TEXTURE)\n",
        "all_three = df['dish_description'].fillna('').str.lower().apply(valid_description).sum()\n",
        "\n",
        "summary = {\n",
        "    'rows': total,\n",
        "    'taste≥1': int(taste_n),\n",
        "    'aroma≥1': int(aroma_n),\n",
        "    'texture≥1': int(texture_n),\n",
        "    'all_three': int(all_three),\n",
        "    'all_three_pct': round(100*all_three/total, 1) if total else 0.0,\n",
        "}\n",
        "print('Coverage:', summary)\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Clean `dish_description` to `dish_description_clean`\n",
        "This section mirrors your cleaning helpers so you can create a cleaned column and write `final.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sarveshaks/Documents/Documents - Sarvesha’s MacBook Pro (2)/Project Stea/project_x/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded '/Users/sarveshaks/Documents/Documents - Sarvesha’s MacBook Pro (2)/Project Stea/project_x/data_cleaned/user_orders_clean_with_description.csv'. Shape: (382, 20)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cleaning progress: 100%|██████████| 382/382 [00:00<00:00, 114102.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning 'dish_description' → 'dish_description_clean'…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cleaning progress: 100%|██████████| 382/382 [00:00<00:00, 8562.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote final CSV to: /Users/sarveshaks/Documents/Documents - Sarvesha’s MacBook Pro (2)/Project Stea/project_x/data_cleaned/final.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import regex, unicodedata\n",
        "from typing import Set, Final, Pattern\n",
        "from tqdm.auto import tqdm as tqdm_auto\n",
        "\n",
        "tqdm_auto.pandas(desc=\"Cleaning progress\")\n",
        "\n",
        "def strip_accents(text: str) -> str:\n",
        "    try:\n",
        "        nfkd_form = unicodedata.normalize(\"NFKD\", str(text))\n",
        "        return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
        "    except TypeError:\n",
        "        return \"\"\n",
        "\n",
        "RE_PAREN: Final[Pattern] = regex.compile(r\"[\\(\\[].*?[\\)\\]]\")\n",
        "NORMALIZE_MAP: Final[dict[str, str]] = {\"w/\": \" with \", \"&\": \" and \"}\n",
        "RE_QUANTITY: Final[Pattern] = regex.compile(\n",
        "    r\"(\\b(serves?|feeds?)\\s*\\d+\\b|\\b\\d+\\s*for\\s*\\$?\\d+(\\.\\d+)?\\b|\\b(x\\d+|\\d+x)\\b|\\b\\d+(\\.\\d+)?\\s*(pc|pcs|ct|oz|lb|lbs|kg|g|ml|l|inch|in|'|\\\")s?\\b|\\b\\d+(\\.\\d+)?s?\\b)\",\n",
        "    flags=regex.IGNORECASE,\n",
        ")\n",
        "RE_INSTRUCTION: Final[Pattern] = regex.compile(\n",
        "    r\"(\\b(no|without|w/o|extra|add|added|light|less|more|sub|swap)\\s+[\\p{L}]+\\b|\\bon\\s+the\\s+side\\b)\",\n",
        "    flags=regex.IGNORECASE,\n",
        ")\n",
        "MARKETING_BLOCKLIST: Final[Set[str]] = {\n",
        "    \"appetizer\",\"add\",\"added\",\"box\",\"bucket\",\"combo\",\"ct\",\"deal\",\"deluxe\",\"dessert\",\"double\",\"entree\",\"family\",\n",
        "    \"feeds\",\"for\",\"g\",\"in\",\"inch\",\"jumbo\",\"junior\",\"kids\",\"kg\",\"l\",\"large\",\"lb\",\"lbs\",\"meal\",\"medium\",\"mini\",\n",
        "    \"ml\",\"oz\",\"pack\",\"party\",\"pc\",\"pcs\",\"platter\",\"regular\",\"serves\",\"single\",\"small\",\"special\",\"starter\",\"tray\",\n",
        "    \"triple\",\"value\",\"xl\",\"xxl\",\"side\"\n",
        "}\n",
        "RE_MARKETING: Final[Pattern] = regex.compile(r\"\\b(\" + r\"|\".join(MARKETING_BLOCKLIST) + r\")\\b\", flags=regex.IGNORECASE)\n",
        "RE_FINAL_SYMBOLS: Final[Pattern] = regex.compile(r\"[^a-z\\s]\")\n",
        "DOMAIN_1CHAR_EXCEPTIONS: Final[Set[str]] = {\"bbq\"}\n",
        "RE_CAPS_INSTRUCTION: Final[Pattern] = regex.compile(r\"\\b([A-Z]{2,}\\s+){1,}[A-Z]{2,}\\b\")\n",
        "\n",
        "def _final_token_cleanup(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    tokens = text.split()\n",
        "    final_tokens = []\n",
        "    last_token = None\n",
        "    for token in tokens:\n",
        "        if token == last_token:\n",
        "            continue\n",
        "        if len(token) > 1 or token in DOMAIN_1CHAR_EXCEPTIONS:\n",
        "            final_tokens.append(token)\n",
        "        last_token = token\n",
        "    return \" \".join(final_tokens)\n",
        "\n",
        "def build_restaurant_blocklist(name: str) -> Set[str]:\n",
        "    if not name or not isinstance(name, str):\n",
        "        return set()\n",
        "    clean_name = strip_accents(name.lower())\n",
        "    clean_name = RE_FINAL_SYMBOLS.sub(\" \", clean_name)\n",
        "    tokens = clean_name.split()\n",
        "    return {token for token in tokens if len(token) >= 3}\n",
        "\n",
        "def clean_text_column(text_str: str, restaurant_tokens: Set[str]) -> str:\n",
        "    if not text_str or not isinstance(text_str, str):\n",
        "        return \"\"\n",
        "    clean_name = strip_accents(text_str.lower())\n",
        "    clean_name = RE_CAPS_INSTRUCTION.sub(\" \", clean_name)\n",
        "    clean_name = RE_PAREN.sub(\" \", clean_name)\n",
        "    for sym, replacement in NORMALIZE_MAP.items():\n",
        "        clean_name = clean_name.replace(sym, replacement)\n",
        "    clean_name = RE_QUANTITY.sub(\" \", clean_name)\n",
        "    clean_name = RE_INSTRUCTION.sub(\" \", clean_name)\n",
        "    clean_name = RE_MARKETING.sub(\" \", clean_name)\n",
        "    if restaurant_tokens:\n",
        "        try:\n",
        "            restaurant_re = regex.compile(r\"\\b(\" + r\"|\".join(regex.escape(token) for token in restaurant_tokens) + r\")\\b\", flags=regex.IGNORECASE)\n",
        "            clean_name = restaurant_re.sub(\" \", clean_name)\n",
        "        except (regex.error, TypeError):\n",
        "            pass\n",
        "    clean_name = RE_FINAL_SYMBOLS.sub(\" \", clean_name)\n",
        "    clean_name = regex.sub(r\"\\s+\", \" \", clean_name).strip()\n",
        "    clean_name = _final_token_cleanup(clean_name)\n",
        "    return clean_name\n",
        "\n",
        "# Load produced file and create dish_description_clean\n",
        "file_path = OUTPUT_CSV\n",
        "df2 = pd.read_csv(file_path)\n",
        "print(f\"Loaded '{file_path}'. Shape: {df2.shape}\")\n",
        "if 'restaurant_tokens' not in df2.columns:\n",
        "    df2['restaurant_tokens'] = df2['restaurant_name'].fillna(\"\").progress_apply(build_restaurant_blocklist)\n",
        "else:\n",
        "    import ast\n",
        "    df2['restaurant_tokens'] = df2['restaurant_tokens'].fillna(\"{}\").progress_apply(ast.literal_eval)\n",
        "\n",
        "print(\"Cleaning 'dish_description' → 'dish_description_clean'…\")\n",
        "df2['dish_description'] = df2['dish_description'].fillna(\"\")\n",
        "df2['dish_description_clean'] = df2.progress_apply(\n",
        "    lambda row: clean_text_column(row['dish_description'], row['restaurant_tokens']), axis=1\n",
        ")\n",
        "OUTPUT_CSV2 = DATA_CLEANED / 'final.csv'\n",
        "df2.to_csv(OUTPUT_CSV2, index=False)\n",
        "print('Wrote final CSV to:', OUTPUT_CSV2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
